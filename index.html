<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
	"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

	<title>LearnAvro</title>
	
	<style type="text/css">
		body {
  		margin-top: 1.0em;
  		background-color: #9b1f32;
		  font-family: "Helvetica,Arial,FreeSans";
  		color: #ffffff;
    }
    #container {
      margin: 0 auto;
      width: 700px;
    }
		h1 { font-size: 3.8em; color: #64e0cd; margin-bottom: 3px; }
		h1 .small { font-size: 0.4em; }
		h1 a { text-decoration: none }
		h2 { font-size: 1.5em; color: #64e0cd; }
    h3 { text-align: center; color: #64e0cd; }
    a { color: #64e0cd; }
    .description { font-size: 1.2em; margin-bottom: 30px; margin-top: 30px; font-style: italic;}
    .download { float: right; }
		pre { background: #000; color: #fff; padding: 15px;}
    hr { border: 0; width: 80%; border-bottom: 1px solid #aaa}
    .footer { text-align:center; padding-top:30px; font-style: italic; }
	</style>
	
</head>

<body>
  <a href="http://github.com/cloudera/LearnAvro"><img style="position: absolute; top: 0; right: 0; border: 0;" src="http://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub" /></a>

  <div id="container">

    <div class="download">
      <a href="http://github.com/cloudera/LearnAvro/zipball/master">
        <img border="0" width="90" src="http://github.com/images/modules/download/zip.png"></a>
      <a href="http://github.com/cloudera/LearnAvro/tarball/master">
        <img border="0" width="90" src="http://github.com/images/modules/download/tar.png"></a>
    </div>

    <h1><a href="http://github.com/cloudera/LearnAvro">LearnAvro</a>
      <span class="small">by Michael Cafarella (University of Michigan), <a href="http://github.com/cloudera">Cloudera</a></span></h1>

    <div class="description">
      LearnAvro: Automatic structure for your HDFS data.
    </div>

   LearnAvro is a project that automatically turns your text-formatted data (logs, sensor readings, etc) into structured Avro data, without any need to write parsers or extractors.  Its goal is to dramatically reduce the time spent preparing data for analysis, enabling more time for the analysis itself.<p>

<h2>Introduction</h2>
    <p>
    Hadoop's HDFS is often used to store large amounts of text-formatted data: log files, sensor readings, transaction histories, etc.  Much of this data is "near-structured": the data has a format that's obvious to a human observer, but is not made explicit in the file itself.  For example, the following line is an example of the <a href="http://en.wikipedia.org/wiki/Common_Log_Format">Common Log Format</a>, often used in web servers:
<p>
<tt>
 127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] "GET /apache_pb.gif HTTP/1.0" 200 2326
</tt>
<p>
It contains a source IP address, followed by a user id, followed by an date and timestamp, then an HTTP request, etc.<p>

When a user wants to process such "near-structured" data with MapReduce, Pig, or some similar tool, she must laboriously reconstruct the metadata that is obvious to anyone who just <i> eyeballs the data</i>.  Performing this reconstruction usually entails writing a parser or extractor, often one based on relatively brittle regular expressions.  It's true that the Common Log Format is so, uh, <i>common</i> that writing a single good parser for it is probably worthwhile.  However, there are also file listings, album track listings, temperature readings, flight schedules, and many many other kinds of data; the number of good parsers we need to write gets large, quickly.  Writing all of these straightforward extractors, again and again, is a time-consuming and error-prone pain for everyone.  We believe it is a major obstacle to faster and easier data analytics<p>

The LearnAvro project aims to <i>automatically generate structure</i> for text-embedded data.  It consists of two main components:
<ol>
<li><b>LearnAvro</b> takes a text file as input and derives a parser that breaks lines of the file into typed fields.  For example, the above web log entry is broken into <tt>127.0.0.1</tt>, followed by <tt>frank</tt>, etc.
<li><b>SchemaDictionary</b> takes data that's been parsed by LearnAvro and applies topic-specific labels.  For example, <tt>127.0.0.1</tt> gets labelled as <i>client ip address</i>, and <tt>frank</tt> is labelled as <tt>user id</tt>.
</ol>

    As you can probably guess, the target structured data format is <a href="http://avro.apache.org/">Avro</a>.  Avro allows efficient cross-platform data serialization, similar to <a href="http://incubator.apache.org/thrift/">Thrift</a> or <a href="http://code.google.com/p/protobuf/">Protocol Buffers</a>.  Data stored in Avro has many advantages (read this <a href="http://www.facebook.com/note.php?note_id=167777112002">overview of Avro</a> for more</a>) and many tools support Avro directly: <a href="http://hadoop.apache.org/mapreduce/">Hadoop MapReduce</a>, <a href="http://hbase.apache.org/">HBase</a>, <a href="http://pig.apache.org/">Pig</a>, and others.<p>
    </p>

<h2>Related Work</h2>
  Our work on the LearnAvro component draws inspiration from the PADS research project (<a href="">http://www.padsproj.org/index.html</a>), in particular this <a href="http://www.padsproj.org/papers/popl08.pdf">2008 POPL paper</a> by Fisher, Walker, Zhu, and White (which itself draws on many papers in the area of information extraction and related fields).  The authors have released code for their system, written in ML.  ML is a great language, but not one that traditionally draws a large open-source community.<p>

  SchemaDictionary is more generally inspired by database schema mapping systems.  (<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.1842&rep=rep1&type=pdf">Clio</a> from Miller, et al., is a famous example.)  Schema mapping systems are usually designed to help database administrators merge existing databases; for example, when company A purchases company B and must then merge the employee lists.  These tools are often expensive and expect a lot of administrator attention.  In contrast, our SchemaDictionary is for busy data analysts who simply want to check out a novel dataset as quickly as possible.  It is fast and simple, but can only handle relatively simple structures (rendering it inappropriate for databases, but on target for the kind of data that is popular in text-based formats).<p>

<h2>Walkthrough</h2>
Imagine you have a very simple log of accesses to your site, stored in <tt>accesses.txt</tt>:<p>
<tt>
February 15, 2011 10:20:10 http://domain.com/post1.html 2192<br>
February 16, 2011 11:34:42 http://domain.com/post2.html 3074<br>
February 16, 2011 22:17:09 http://domain.com/post2.html 3074<br>
</tt>
<p>

We can now run our tools to automatically turn this text file into a structured Avro file.<p>

<h3>LearnAvro</h3>
We run the first component, LearnAvro, as follows:<p>
<tt>
$ bin/learnavro learn accesses.txt outdir
</tt><p>

This tells the learnavro tool to learn the Avro structure found in <tt>accesses.txt</tt>, and to write it out to the <tt>outdir</tt> directory.  <tt>outdir</tt> contains three files: <tt>schema.json</tt>, <tt>data.avro</tt>, and <tt>parser.dat</tt>.<p>

The most interesting is probably <tt>schema.json</tt>, the program's attempt at building a JSON schema that describes the text data.  In the case of the example above, it looks like this:<p>
<code>
{<br>
&nbsp;  "type" : "record",<br>
&nbsp;  "name" : "record-1",<br>
&nbsp;  "namespace" : "",<br>
&nbsp;  "fields" : [ {<br>
&nbsp;&nbsp;    "name" : "base-0",<br>
&nbsp;&nbsp;    "type" : {<br>
&nbsp;&nbsp;&nbsp;      "type" : "record",<br>
&nbsp;&nbsp;&nbsp;      "fields" : [ {<br>
&nbsp;&nbsp;&nbsp;&nbsp;        "name" : "month",<br>
&nbsp;&nbsp;&nbsp;&nbsp;        "type" : "int"<br>
&nbsp;&nbsp;&nbsp;      }, {<br>
&nbsp;&nbsp;&nbsp;&nbsp;      "name" : "day",<br>
&nbsp;&nbsp;&nbsp;&nbsp;        "type" : "int"<br>
&nbsp;&nbsp;&nbsp;      }, {<br>
&nbsp;&nbsp;&nbsp;&nbsp;        "name" : "year",<br>
&nbsp;&nbsp;&nbsp;&nbsp;        "type" : "int"<br>
&nbsp;&nbsp;&nbsp;      } ]<br>
&nbsp;    }<br>
&nbsp;  }, {<br>
&nbsp;&nbsp;    "name" : "base-2",<br>
&nbsp;&nbsp;    "type" : {<br>
&nbsp;&nbsp;&nbsp;      "type" : "record",<br>
&nbsp;&nbsp;&nbsp;      "fields" : [ {<br>
&nbsp;&nbsp;&nbsp;&nbsp;        "name" : "hrs",<br>
&nbsp;&nbsp;&nbsp;&nbsp;        "type" : "int"<br>
&nbsp;&nbsp;&nbsp;      }, {<br>
&nbsp;&nbsp;&nbsp;&nbsp;        "name" : "mins",<br>
&nbsp;&nbsp;&nbsp;&nbsp;        "type" : "int"<br>
&nbsp;&nbsp;&nbsp;      }, {<br>
&nbsp;&nbsp;&nbsp;&nbsp;        "name" : "secs",<br>
&nbsp;&nbsp;&nbsp;&nbsp;        "type" : "int"<br>
&nbsp;&nbsp;&nbsp;      } ]<br>
&nbsp;    }<br>
&nbsp;  }, {<br>
&nbsp;&nbsp;    "name" : "base-4",<br>
&nbsp;&nbsp;    "type" : "string"<br>
&nbsp;  }, {<br>
&nbsp;&nbsp;    "name" : "base-6",<br>
&nbsp;&nbsp;    "type" : "int"<br>
&nbsp;  } ]<br>
}<br>
</code>

The JSON starts by describing two records, each consisting of three integers.  The first record has fields <tt>month</tt>, <tt>day</tt>, and <tt>year</tt> and clearly describes the date that starts every line in <tt>accesses.txt</tt>.  The second record represents a timestamp, with fields <tt>hrs</tt>, <tt>mins</tt>, and <tt>secs</tt>.  Following these two records are a string and an integer, matching the URL and file size from the raw input file.<p>

All four of the top-level data values -- the date record, the timestamp record, the URL string, and the file size integer -- have nondescriptive synthetically-generated names that begin with <tt>base-</tt>.  The LearnAvro step discerns the type of each field, but not its role.  Obtaining names for these fields is the job of the SchemaDictionary.  For now, we just live with the synthetic names.<p>

Next, let's look at <tt>data.avro</tt>, the actual Avro data.  We currently use a textual version of Avro data for clarity and debugging reasons, but we could switch to binary quite easily.  The contents of <tt>data.avro</tt> look like this:<p>
<tt>
{"base-0":{"month":-1,"day":15,"year":2011},"base-2":{"hrs":10,"mins":30,"secs":12},"base-4":"http://domain.com/post1.html","base-6":2034} {"base-0":{"month":-1,"day":16,"year":2011},"base-2":{"hrs":9,"mins":42,"secs":57},"base-4":"http://domain.com/post2.html","base-6":3147} {"base-0":{"month":-1,"day":16,"year":2011},"base-2":{"hrs":22,"mins":12,"secs":54},"base-4":"http://domain.com/post2.html","base-6":3147}
</tt>
<p>

The field names here reflect the contents of <tt>schema.json</tt>, and the values of those fields reflect the contents of <tt>accesses.txt</tt>.<p>

The final LearnAvro output file is <tt>parser.dat</tt>.  This is a binary representation of the parser generated by LearnAvro; the program applied this parser to <tt>accesses.txt</tt> in order to obtain <tt>data.avro</tt>.  If the user wants to process more data that has the same format as <tt>accesses.txt</tt>, there's no need to relearn the structure; she can simply reapply the already-learned parser.<p>

We're now ready to apply the SchemaDictionary component.<p>

<h3>SchemaDictionary</h3>

Most of the time, we could now use the SchemaDictionary tool to compare the structured but anonymous <tt>data.avro</tt> and <tt>schema.json</tt> against a large number of known data types.  The tool finds the closest match and uses that match to generate labels for the anonymous fields in our new Avro file.<p>

Unfortunately, we don't yet have any known data types.  Before we can compare LearnAvro's output to anything, we need to assemble a small set of known types.  Typing in the following commands:<p>

<tt>bin/schemadict dict -m "Simple web access log" -a src/samples/schemas/web-data2.avro schemaDict</tt><br>
<tt>bin/schemadict dict -m "Local bus schedule" -a src/samples/schemas/bus-schedule.avro schemaDict</tt><p>

will create a new dictionary called <tt>schemaDict</tt>.  We will add to it two known data samples stored in <tt>src/samples/schemas</tt>, supplying a small comment to describe each one.  We can then display the contents of <tt>schemaDict</tt> with this command:<p>

<tt>bin/schemadict dict -d schemaDict</tt><p>

which should confirm that <tt>schemaDict</tt> contains the two data types seen above.  We're now ready to apply labels to our original file from LearnAvro.  Entering this command:<p>

<tt>bin/schemadict learn schemaDict outdir</tt><p>

will ask the SchemaDictionary tool to learn a labelset for <tt>schemaDict</tt>, using the known datatypes in <tt>outdir</tt>.  It outputs the following:<p>



<h2>Dependencies</h2>
<p>I depend on nothing!</p>
<h2>Install</h2>
<p>Install me...</p>
<h2>License</h2>
<p>Apache 2.0 </p>
<h2>Authors</h2>
<p>Mike Cafarella (michjc@umich.edu)<br/><br/>      </p>
<h2>Contact</h2>
<p>Cloudera (github@cloudera.com)<br/>      </p>


    <h2>Download</h2>
    <p>
      You can download this project in either
      <a href="http://github.com/cloudera/LearnAvro/zipball/master">zip</a> or
      <a href="http://github.com/cloudera/LearnAvro/tarball/master">tar</a> formats.
    </p>
    <p>You can also clone the project with <a href="http://git-scm.com">Git</a>
      by running:
      <pre>$ git clone git://github.com/cloudera/LearnAvro</pre>
    </p>

    <div class="footer">
      get the source code on GitHub : <a href="http://github.com/cloudera/LearnAvro">cloudera/LearnAvro</a>
    </div>

  </div>

  
</body>
</html>
